{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678ab27a",
   "metadata": {},
   "source": [
    "This notebook creates the following tables:\n",
    "\n",
    "- roster (final_team_roster.csv)\n",
    "- teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ffd10",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd74a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import config\n",
    "\n",
    "config.email = \"rde6mn@virginia.edu\"\n",
    "config.max_retries = 5\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8627d",
   "metadata": {},
   "source": [
    "## Combine all separate team roster excels into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the folder containing the Excel files\n",
    "folder_path = Path(\"Data/Just Teams\")\n",
    "\n",
    "# Get a list of all Excel files in the folder\n",
    "excel_files = list(folder_path.glob(\"*.xlsx\"))\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file\n",
    "for idx, file in enumerate(excel_files):\n",
    "    # Read only the first 7 columns\n",
    "    df = pd.read_excel(file, usecols=range(7), sheet_name=0)\n",
    "    \n",
    "    # For the first file, keep the original headers\n",
    "    if idx == 0:\n",
    "        combined_df = df.copy()\n",
    "        combined_df['Team'] = file.stem  # Add 'Team' column using filename\n",
    "    else:\n",
    "        df.columns = combined_df.columns[:-1]  # Reuse headers from first file (excluding 'Team')\n",
    "        df['Team'] = file.stem\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Final combined dataframe: combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376199e",
   "metadata": {},
   "source": [
    "## Obtain OpenAlexID value using ORCID value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming Authors() returns a dictionary-like object you can index with ORCID URLs\n",
    "# If it's a function that does live API calls, make sure it's efficient or cached\n",
    "author_data = Authors()  # preload or initialize API access\n",
    "\n",
    "# Add new column to hold OpenAlex ID suffix\n",
    "combined_df['OpenAlex_ID'] = None\n",
    "\n",
    "# Dictionary to store full JSON objects for matched authors\n",
    "orcid_json_dict = {}\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for idx, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    orcid_id = row['ORCID']\n",
    "    full_name = f\"{row['First Name']} {row['Last Name']}\".strip()\n",
    "\n",
    "    orcid_url = f\"https://orcid.org/{orcid_id}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch data\n",
    "        author_json = author_data[orcid_url]\n",
    "\n",
    "        # Compare names (case-insensitive match)\n",
    "        if author_json.get('display_name', '').strip().lower() == full_name.lower():\n",
    "            # Extract suffix of OpenAlex ID\n",
    "            openalex_suffix = author_json['id'].split('/')[-1]\n",
    "\n",
    "            # Store in DataFrame\n",
    "            combined_df.at[idx, 'OpenAlex_ID'] = openalex_suffix\n",
    "\n",
    "            # Store full JSON object in dictionary\n",
    "            orcid_json_dict[orcid_id] = author_json\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ORCID {orcid_id}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9784deb",
   "metadata": {},
   "source": [
    "## Add missing lab values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a984bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.read_csv(\"Data/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['Lab'] = str(cleaned_df['Lab'])\n",
    "cleaned_df['First_Name'] = str(cleaned_df['First_Name'])\n",
    "cleaned_df['Last_Name'] = str(cleaned_df['Last_Name'])\n",
    "combined_df['Lab'] = str(combined_df['Lab'])\n",
    "combined_df['First Name'] = str(combined_df['First Name'])\n",
    "combined_df['Last Name'] = str(combined_df['Last Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.columns = cleaned_df.columns.str.strip()\n",
    "combined_df.columns = combined_df.columns.str.strip()\n",
    "\n",
    "# Create lowercase copies for matching (don't modify originals)\n",
    "combined_df['_fn_lc'] = combined_df['First Name'].astype(str).str.strip().str.lower()\n",
    "combined_df['_ln_lc'] = combined_df['Last Name'].astype(str).str.strip().str.lower()\n",
    "combined_df['_orcid_lc'] = combined_df['ORCID'].astype(str).str.strip().str.lower()\n",
    "combined_df['_lab_lc'] = combined_df['Lab'].astype(str).str.strip().str.lower()\n",
    "\n",
    "cleaned_df['_fn_lc'] = cleaned_df['First_Name'].astype(str).str.strip().str.lower()\n",
    "cleaned_df['_ln_lc'] = cleaned_df['Last_Name'].astype(str).str.strip().str.lower()\n",
    "cleaned_df['_orcid_lc'] = cleaned_df['ORCID'].astype(str).str.strip().str.lower()\n",
    "cleaned_df['_lab_lc'] = cleaned_df['Lab'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Count missing Lab values before\n",
    "missing_before = combined_df['Lab'].isna().sum()\n",
    "print(f\"Missing 'Lab' values before update: {missing_before}\")\n",
    "\n",
    "# Fill missing Lab\n",
    "missing_lab_mask = combined_df['Lab'].isna()\n",
    "\n",
    "for idx, row in combined_df[missing_lab_mask].iterrows():\n",
    "    fn, ln, orcid = row['_fn_lc'], row['_ln_lc'], row['_orcid_lc']\n",
    "\n",
    "    match = cleaned_df[\n",
    "        (cleaned_df['_fn_lc'] == fn) &\n",
    "        (cleaned_df['_ln_lc'] == ln) &\n",
    "        (cleaned_df['_orcid_lc'] == orcid)\n",
    "    ]\n",
    "\n",
    "    if not match.empty:\n",
    "        combined_df.at[idx, 'Lab'] = match.iloc[0]['Lab']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85442853",
   "metadata": {},
   "source": [
    "## Pull orcid from cleaned_df if missing from combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'OtherORCID' column exists\n",
    "if 'OtherORCID' not in combined_df.columns:\n",
    "    combined_df['OtherORCID'] = pd.NA\n",
    "\n",
    "# Process only rows with missing OpenAlex_ID\n",
    "missing_openalex_mask = combined_df['OpenAlex_ID'].isna()\n",
    "\n",
    "for idx, row in combined_df[missing_openalex_mask].iterrows():\n",
    "    fn = row['_fn_lc']\n",
    "    ln = row['_ln_lc']\n",
    "    lab = row['_lab_lc']\n",
    "\n",
    "    # Find match in cleaned_df\n",
    "    match = cleaned_df[\n",
    "        (cleaned_df['_fn_lc'] == fn) &\n",
    "        (cleaned_df['_ln_lc'] == ln) &\n",
    "        (cleaned_df['_lab_lc'] == lab)\n",
    "    ]\n",
    "\n",
    "    if not match.empty:\n",
    "        combined_df.at[idx, 'OtherORCID'] = match.iloc[0]['ORCID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(columns=['_fn_lc', '_ln_lc', '_lab_lc', '_orcid_lc'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25dffb",
   "metadata": {},
   "source": [
    "## For rows missing OpenAlexID, cross-ref with cleaned_df. If first name, last name and lab match, use the ORCID value to pull an OpenAlexID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure OtherAlex_ID column exists\n",
    "combined_df['OtherAlex_ID'] = pd.NA\n",
    "\n",
    "# Instantiate the API object\n",
    "authors_api = Authors()\n",
    "\n",
    "# Filter rows where OpenAlex_ID is missing and OtherORCID is not equal to ORCID\n",
    "mask = combined_df['OpenAlex_ID'].isna() & (\n",
    "    combined_df['OtherORCID'].notna() & (combined_df['OtherORCID'] != combined_df['ORCID'])\n",
    ")\n",
    "\n",
    "for idx, row in combined_df[mask].iterrows():\n",
    "    orcid = row['OtherORCID']\n",
    "    try:\n",
    "        author_json = authors_api[f\"https://orcid.org/{orcid}\"]\n",
    "        openalex_id_full = author_json.get('id', '')\n",
    "\n",
    "        if openalex_id_full.startswith(\"https://openalex.org/\"):\n",
    "            openalex_id_suffix = openalex_id_full.split(\"https://openalex.org/\")[-1]\n",
    "            combined_df.at[idx, 'OtherAlex_ID'] = openalex_id_suffix\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ORCID {orcid} at index {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac909f2",
   "metadata": {},
   "source": [
    "## Combine OpenAlex_ID and OtherAlex_ID to create a column with verified ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19530209",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['VerifiedAlex_IDS'] = combined_df['OpenAlex_ID'].fillna(combined_df['OtherAlex_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096c37e",
   "metadata": {},
   "source": [
    "## For rows missing VerifiedAlex_IDS, use the ORCID to pull from OpenAlex API without checking for name matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ed3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure column exists\n",
    "combined_df['UnverifiedAlex_ID'] = pd.NA\n",
    "\n",
    "# Instantiate OpenAlex API\n",
    "authors_api = Authors()\n",
    "\n",
    "# Identify target rows\n",
    "mask = combined_df['VerifiedAlex_IDS'].isna() & combined_df['ORCID'].notna()\n",
    "\n",
    "# Loop through applicable rows\n",
    "for idx, row in combined_df[mask].iterrows():\n",
    "    orcid = row['ORCID']\n",
    "    try:\n",
    "        author_json = authors_api[f\"https://orcid.org/{orcid}\"]\n",
    "        openalex_url = author_json.get('id', '')\n",
    "        if openalex_url.startswith(\"https://openalex.org/\"):\n",
    "            suffix = openalex_url.split(\"https://openalex.org/\")[-1]\n",
    "            combined_df.at[idx, 'UnverifiedAlex_ID'] = suffix\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for ORCID {orcid} at index {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cebd8",
   "metadata": {},
   "source": [
    "## Combine verified and unverified open alex ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['AllOpenAlex_ID'] = combined_df['VerifiedAlex_IDS'].fillna(combined_df['UnverifiedAlex_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e062d",
   "metadata": {},
   "source": [
    "## Clean Team Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combined_df = pd.read_csv(\"combined_teams_with_all_openalex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad1f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Team'] = combined_df['Team'].str.replace('Team ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1a7b4",
   "metadata": {},
   "source": [
    "## Merge supplement results onto roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7580b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_df = pd.read_excel(\"Supplement Grants by Team.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082accdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(supplement_df, on='Team', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe175a9",
   "metadata": {},
   "source": [
    "## Merge rounds onto roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03374717",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_df = pd.read_csv(\"ASAP Rounds - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9169a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(rounds_df, on='Team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2097535",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"final_team_roster.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
